# -*- coding: utf-8 -*-
"""modeling

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DYi_QE7_v5TvxnIgpSlPMh-EhNkSi30j
"""

# ============================================================
# 1. IMPORT LIBRARY
# ============================================================

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    classification_report, confusion_matrix, accuracy_score
)

from xgboost import XGBClassifier

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dropout, Dense, Input
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping

from imblearn.over_sampling import SMOTE

print("Library Loaded")

# ============================================================
# 2. LOAD DATASET
# ============================================================

# Jika di Colab, upload file dataset-gempa.csv
from google.colab import files

uploaded = files.upload()
df = pd.read_csv(next(iter(uploaded.keys())))

print("Dataset Shape:", df.shape)
df.head()

# ============================================================
# 3. PREPROCESSING (Filter Tahun, Drop Kolom, Buat Label)
# ============================================================

# The 'time' column is not present in the dataframe, so we remove the
# lines that attempt to convert it or extract 'year' from it.
# The 'year' column already exists and can be directly used for filtering.

# Filter tahun 2020â€“2024
df = df[(df["year"] >= 2020) & (df["year"] <= 2024)].reset_index(drop=True)

# Drop kolom tidak diperlukan
drop_cols = ["id","net","updated","place","type",
             "status","locationSource","magSource","magType","time","nst"]

# Use errors="ignore" to safely drop columns if they don't exist
df = df.drop(columns=[c for c in drop_cols if c in df.columns], errors="ignore")

# Label klasifikasi kedalaman
def depth_to_class(d):
    if d < 70:
        return 0
    elif d < 300:
        return 1
    else:
        return 2

df["depth_class"] = df["depth"].apply(depth_to_class)

print("Distribusi Depth Class:")
df["depth_class"].value_counts()

# ============================================================
# 4. PILIH FITUR
# ============================================================

feature_cols = [
    "year", "latitude", "longitude", "mag",
    "gap", "dmin", "rms",
    "horizontalError", "depthError", "magError"
]

df = df.dropna(subset=feature_cols)

X = df[feature_cols].values
y = df["depth_class"].values

print("X shape:", X.shape)
print("y shape:", y.shape)

# Scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

print("Scaling complete.")

# ============================================================
# 5. FIX IMBALANCE (SMOTE)
# ============================================================

print("Applying SMOTE...")
sm = SMOTE(random_state=42)
X_bal, y_bal = sm.fit_resample(X_scaled, y)

print("Balanced Shape:", X_bal.shape)
print("Balanced Class Distribution:", np.bincount(y_bal))

# ============================================================
# 6. TRAIN-TEST SPLIT
# ============================================================

X_train, X_test, y_train, y_test = train_test_split(
    X_bal, y_bal, test_size=0.2, random_state=42, stratify=y_bal
)

num_classes = len(np.unique(y_bal))
print("Train:", X_train.shape)
print("Test :", X_test.shape)

# ============================================================
# 7. TRAIN XGBOOST
# ============================================================

xgb = XGBClassifier(
    n_estimators=300,
    max_depth=5,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    objective="multi:softprob",
    num_class=num_classes,
    eval_metric="mlogloss",
    random_state=42
)

print("Training XGBoost...")
xgb.fit(X_train, y_train)

y_pred_xgb = xgb.predict(X_test)

print("\nXGBoost Accuracy:", accuracy_score(y_test, y_pred_xgb))
print("\nXGBoost Classification Report:\n", classification_report(y_test, y_pred_xgb))

# Confusion Matrix XGBoost
cm_xgb = confusion_matrix(y_test, y_pred_xgb)
plt.figure(figsize=(5,4))
sns.heatmap(cm_xgb, annot=True, cmap="Greens", fmt="d")
plt.title("Confusion Matrix - XGBoost")
plt.show()

# ============================================================
# 8. TRAIN LSTM
# ============================================================

# Reshape untuk LSTM timesteps=1
X_train_lstm = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))
X_test_lstm  = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))

y_train_cat = to_categorical(y_train, num_classes=num_classes)
y_test_cat  = to_categorical(y_test, num_classes=num_classes)

tf.random.set_seed(42)

model_lstm = Sequential([
    Input(shape=(1, X_train.shape[1])),
    LSTM(64, return_sequences=True),
    Dropout(0.3),
    LSTM(32),
    Dropout(0.3),
    Dense(32, activation="relu"),
    Dense(num_classes, activation="softmax")
])

model_lstm.compile(
    optimizer="adam",
    loss="categorical_crossentropy",
    metrics=["accuracy"]
)

early_stop = EarlyStopping(
    monitor="val_loss",
    patience=5,
    restore_best_weights=True
)

print("Training LSTM...")
history = model_lstm.fit(
    X_train_lstm, y_train_cat,
    validation_split=0.2,
    epochs=50,
    batch_size=64,
    callbacks=[early_stop],
    verbose=1
)

# LOSS GRAPH
plt.figure(figsize=(6,4))
plt.plot(history.history["loss"], label="Train Loss")
plt.plot(history.history["val_loss"], label="Val Loss")
plt.title("Loss LSTM")
plt.legend()
plt.show()

# ACC GRAPH
plt.figure(figsize=(6,4))
plt.plot(history.history["accuracy"], label="Train Acc")
plt.plot(history.history["val_accuracy"], label="Val Acc")
plt.title("Accuracy LSTM")
plt.legend()
plt.show()

# ============================================================
# 9. EVALUASI LSTM
# ============================================================

y_pred_lstm = np.argmax(model_lstm.predict(X_test_lstm), axis=1)

print("\nLSTM Accuracy:", accuracy_score(y_test, y_pred_lstm))
print("\nLSTM Classification Report:\n", classification_report(y_test, y_pred_lstm))

cm_lstm = confusion_matrix(y_test, y_pred_lstm)

plt.figure(figsize=(5,4))
sns.heatmap(cm_lstm, annot=True, cmap="Purples", fmt="d")
plt.title("Confusion Matrix - LSTM")
plt.show()

# ============================================================
# 10. SAVE MODEL
# ============================================================

import joblib

os.makedirs("models", exist_ok=True)

joblib.dump(scaler, "models/scaler.pkl")
joblib.dump(xgb, "models/xgb_depth_class.pkl")
model_lstm.save("models/lstm_depth_class.keras")

print("All models saved in /models folder")