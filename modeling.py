# -*- coding: utf-8 -*-
"""modeling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_yhY0ulLgX5ZCWC4FhQVMyMZFz4LZfzS

# PROJECT AKHIR - KLASIFIKASI KEDALAMAN GEMPA
# MENGGUNAKAN LSTM DAN XGBOOST

# **1. IMPORT LIBRARY**


Pada bagian ini dilakukan pemanggilan seluruh library yang digunakan dalam analisis, visualisasi, dan pemodelan.
- **NumPy, Pandas**: manipulasi data
- **Matplotlib, Seaborn**: visualisasi
- **Scikit-learn**: preprocessing, training model, evaluasi
- **XGBoost**: model gradient boosting
"""

# ============================================================
# 1. IMPORT LIBRARY
# ============================================================
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    confusion_matrix
)
from sklearn.preprocessing import StandardScaler
from sklearn.utils.class_weight import compute_class_weight

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import to_categorical

from xgboost import XGBClassifier
import joblib
import tensorflow as tf

# Opsional: supaya hasil bisa direpro
np.random.seed(42)
tf.random.set_seed(42)

"""# **3. LOAD DATASET**


Dataset dibaca menggunakan `pandas.read_csv()`. Kolom waktu dikonversi ke format datetime agar bisa diekstrak tahun dan dianalisis lebih lanjut.
"""

pd.read_csv("dataset_gempa.csv")

# Konversi kolom waktu ke datetime
df["time"] = pd.to_datetime(df["time"], errors="coerce")

print("5 data teratas:")
print(df.head())
print("\nJumlah kolom:", len(df.columns))

"""# **4. FILTER TAHUN**


Analisis dibatasi pada 5 tahun terakhir agar model mempelajari pola terbaru dan mengurangi noise dari data lama. 2020–2024
"""

# Ekstrak tahun dari kolom time
df["year"] = df["time"].dt.year

# Filter tahun
df = df[(df["year"] >= 2020) & (df["year"] <= 2024)].reset_index(drop=True)

# Drop kolom yang tidak digunakan
drop_cols = [
    "id", "net", "updated", "place", "type",
    "status", "locationSource", "magSource", "magType",
    "time",      # sudah diambil 'year'
    "nst"        # banyak missing
]

existing_cols_to_drop = [col for col in drop_cols if col in df.columns]
df = df.drop(columns=existing_cols_to_drop)

print("Kolom tersisa setelah drop:")
print(df.columns)
print("Shape setelah filter tahun:", df.shape)

"""# **6. MEMBUAT LABEL**



Kedalaman dikategorikan menjadi 3 kelas:
- 0 = Shallow (<70 km)
- 1 = Intermediate (70–300 km)
- 2 = Deep (>300 km)

Kelas	Kedalaman	Tingkat Bahaya	Alasan:

- 0 (Shallow)	< 70 km	Paling berbahaya	Energi tidak banyak melemah, getaran sangat kuat

- 1 (Intermediate)	70–300 km	sedang	Energi melemah sebagian, masih bisa dirasakan luas

- 2 (Deep)	> 300 km	 Paling aman	Energi paling banyak teredam sebelum mencapai permukaan

"""

def depth_to_class(depth):
    if depth < 70:
        return 0   # shallow
    elif depth < 300:
        return 1   # intermediate
    else:
        return 2   # deep

df["depth_class"] = df["depth"].apply(depth_to_class)
print(df[["depth", "depth_class"]].head())

# ============================================================
# 6. MENATA URUTAN KOLOM
#    - 'year' dijadikan kolom pertama
#    - 'depth_class' diletakkan di kolom terakhir
# ============================================================

cols = (
    ["year"] +
    [c for c in df.columns if c not in ["year", "depth_class"]] +
    ["depth_class"]
)
df = df[cols]

print("Urutan kolom baru:")
print(df.columns)

"""# **5. DATA UNDERSTANDING**

Tahap ini bertujuan memahami struktur dan karakteristik data, meliputi:

- Informasi tipe data
- Statistik deskriptif
- Jumlah missing values
"""

print("\nInformasi dataset:")
print(df.info())

print("\nStatistik deskriptif (fitur numerik):")
print(df.describe())

print("\nMissing values:")
print(df.isna().sum())

"""- Grafik histogram digunakan untuk melihat persebaran kedalaman gempa. Mayoritas gempa cenderung dangkal (<70 km).
- Scatter plot latitude–longitude menampilkan persebaran episenter gempa di Indonesia.
"""

# Histogram kedalaman
plt.figure(figsize=(6,4))
sns.histplot(df["depth"], bins=50, kde=True)
plt.title("Distribusi Kedalaman Gempa")
plt.xlabel("Depth (km)")
plt.ylabel("Frekuensi")
plt.show()

# Sebaran lokasi
plt.figure(figsize=(6,5))
plt.scatter(df["longitude"], df["latitude"], s=5, alpha=0.5)
plt.title("Sebaran Lokasi Gempa 2020–2024")
plt.xlabel("Longitude")
plt.ylabel("Latitude")
plt.show()

# Distribusi kelas kedalaman
plt.figure(figsize=(4,3))
sns.countplot(x=df["depth_class"])
plt.title("Distribusi Kelas Kedalaman Gempa")
plt.xlabel("0=Shallow, 1=Intermediate, 2=Deep")
plt.ylabel("Jumlah")
plt.show()

nama_file = "gempabumi-fiks.csv"
df.to_csv(nama_file, index=False)
print(f"DataFrame berhasil disimpan sebagai {nama_file}")

"""# **7. PEMILIHAN FITUR**

Fitur yang dipilih meliputi parameter lokasi, magnitude, hingga error pengukuran. Missing values diganti menggunakan median.Heatmap digunakan untuk melihat hubungan antar variabel dan mengidentifikasi potensi redundansi.

"""

feature_cols = [
    "year", "latitude", "longitude", "mag",
    "gap", "dmin", "rms",
    "horizontalError", "depthError", "magError",
]

# Isi missing value (kalau ada) dengan median
df[feature_cols] = df[feature_cols].fillna(df[feature_cols].median())

X = df[feature_cols]
y = df["depth_class"]

print("Shape X:", X.shape)
print("Shape y:", y.shape)
print("Contoh X.head():")
print(X.head())

plt.figure(figsize=(10,7))
sns.heatmap(df[feature_cols].corr(), cmap="coolwarm")
plt.title("Correlation Heatmap Fitur Gempa")
plt.show()

"""# **8. TRAIN-TEST SPLIT**

Dataset dibagi menjadi 80% data training dan 20% data testing, dengan stratifikasi agar proporsi kelas tetap seimbang.

"""

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    stratify=y,
    random_state=42
)

print("Train shape:", X_train.shape)
print("Test shape :", X_test.shape)

"""# **9. CLASS WEIGHT**

Karena dataset tidak seimbang (kelas deep sedikit), dihitung bobot untuk mengurangi bias model.

"""

classes = np.unique(y_train)
cw = compute_class_weight(
    class_weight="balanced",
    classes=classes,
    y=y_train
)
class_weights = dict(zip(classes, cw))

print("Class weights:", class_weights)

"""# **10. MODEL LSTM**

Model Random Forest dilatih dengan class weight untuk menangani imbalance dan diuji menggunakan classification report.Feature Importance (Random Forest)
Menunjukkan fitur mana yang memiliki kontribusi paling besar pada prediksi model.
"""

# Persiapan data untuk LSTM
# LSTM butuh input 3D: (samples, timesteps, features)
# Di sini kita pakai time_steps = 1 (tiap baris dianggap 1 langkah waktu)


scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled  = scaler.transform(X_test)

# LSTM butuh input 3D: (samples, timesteps, features)
# Di sini time_steps = 1 (setiap baris dianggap satu langkah waktu)
X_train_lstm = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))
X_test_lstm  = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))

num_classes = len(np.unique(y))
y_train_cat = to_categorical(y_train, num_classes=num_classes)
y_test_cat  = to_categorical(y_test, num_classes=num_classes)

print("X_train_lstm shape:", X_train_lstm.shape)
print("y_train_cat shape  :", y_train_cat.shape)

# Model LSTM
tf.random.set_seed(42)

model_lstm = Sequential([
    LSTM(64, return_sequences=True, input_shape=(1, X_train_lstm.shape[2])),
    Dropout(0.3),
    LSTM(32),
    Dropout(0.3),
    Dense(32, activation="relu"),
    Dense(num_classes, activation="softmax")
])

model_lstm.compile(
    loss="categorical_crossentropy",
    optimizer="adam",
    metrics=["accuracy"]
)

model_lstm.summary()

# Training LSTM
early_stop = EarlyStopping(
    monitor="val_loss",
    patience=5,
    restore_best_weights=True
)

history = model_lstm.fit(
    X_train_lstm, y_train_cat,
    validation_split=0.2,
    epochs=50,
    batch_size=64,
    class_weight=class_weights,
    callbacks=[early_stop],
    verbose=1
)

# Evaluasi Model LSTM

# Plot loss
plt.figure(figsize=(6,4))
plt.plot(history.history["loss"], label="Train Loss")
plt.plot(history.history["val_loss"], label="Val Loss")
plt.title("Training vs Validation Loss (LSTM)")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.show()

# Plot accuracy
plt.figure(figsize=(6,4))
plt.plot(history.history["accuracy"], label="Train Acc")
plt.plot(history.history["val_accuracy"], label="Val Acc")
plt.title("Training vs Validation Accuracy (LSTM)")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

# Prediksi di data test
y_pred_proba_lstm = model_lstm.predict(X_test_lstm)
y_pred_lstm = np.argmax(y_pred_proba_lstm, axis=1)

print("=== LSTM (Klasifikasi Kedalaman) ===")
print("Accuracy (test) :", accuracy_score(y_test, y_pred_lstm))
print("\nClassification Report LSTM:")
print(classification_report(y_test, y_pred_lstm, digits=4))

# Confusion matrix
cm_lstm = confusion_matrix(y_test, y_pred_lstm)
plt.figure(figsize=(5,4))
sns.heatmap(cm_lstm, annot=True, fmt='d', cmap="Purples",
            xticklabels=[0,1,2], yticklabels=[0,1,2])
plt.title("Confusion Matrix - LSTM")
plt.xlabel("Prediksi")
plt.ylabel("Aktual")
plt.show()

"""# **11. MODEL XGBOOST**

Model gradient boosting yang biasanya lebih kuat dan akurat dibanding Random Forest. Menggunakan sample weight untuk mengatasi imbalance. Visualisasi ini menjelaskan fitur apa yang paling berpengaruh dalam model XGBoost.

"""

# Sample weight berdasarkan class_weight
sample_weight = y_train.map(class_weights)

xgb = XGBClassifier(
    n_estimators=300,
    max_depth=5,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    objective="multi:softmax",
    num_class=num_classes,
    eval_metric="mlogloss",
    random_state=42
)

xgb.fit(X_train, y_train, sample_weight=sample_weight)
y_pred_xgb = xgb.predict(X_test)

print("=== XGBoost (Klasifikasi Kedalaman) ===")
print("XGBoost Accuracy (test):", accuracy_score(y_test, y_pred_xgb))
print("\nClassification Report XGBoost:")
print(classification_report(y_test, y_pred_xgb, digits=4))

cm_xgb = confusion_matrix(y_test, y_pred_xgb)
plt.figure(figsize=(5,4))
sns.heatmap(cm_xgb, annot=True, cmap="Greens",
            xticklabels=[0,1,2], yticklabels=[0,1,2])
plt.title("Confusion Matrix - XGBoost")
plt.xlabel("Prediksi")
plt.ylabel("Aktual")
plt.show()

"""# **14. SIMPAN MODEL**

Model disimpan dalam format `.pkl` agar dapat digunakan kembali untuk prediksi tanpa perlu dilatih ulang.

"""

os.makedirs("models", exist_ok=True)

joblib.dump(scaler, "models/scaler.pkl")
model_lstm.save("models/lstm_depth_class.keras")
joblib.dump(xgb, "models/xgb_depth_class.pkl")

print("Model dan scaler berhasil disimpan di folder 'models'.")

"""# **Kesimpulan :**

Penelitian ini berhasil melakukan klasifikasi kedalaman gempa bumi ke dalam tiga kelas shallow, intermediate, dan deep menggunakan algoritma LSTM dan XGBoost. Data gempa periode 2020–2024 menunjukkan dominasi gempa dangkal sehingga diperlukan penanganan imbalance melalui penggunaan class weight dan sample weight. Analisis fitur memperlihatkan bahwa latitude, longitude, magnitudo, dan depthError merupakan variabel yang paling berpengaruh dalam menentukan kedalaman hiposenter. Hasil pelatihan menunjukkan bahwa LSTM mampu mempelajari pola non-linear dan memberikan akurasi validasi sekitar 76%. Namun, XGBoost memberikan performa yang lebih stabil dan akurat, terutama dalam memprediksi kelas deep yang jumlahnya paling sedikit. Secara keseluruhan, XGBoost menjadi model terbaik dalam klasifikasi kedalaman gempa berbasis data tabular. Model yang dihasilkan telah berhasil disimpan dan dapat digunakan kembali untuk prediksi pada data baru melalui sistem berbasis web.

*Lailasalsabillahanifa-202210715333*
"""
