# -*- coding: utf-8 -*-
"""modeling

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DYi_QE7_v5TvxnIgpSlPMh-EhNkSi30j
"""

# 1. IMPORT LIBRARY

import os
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import (
    classification_report, confusion_matrix, accuracy_score
)

from xgboost import XGBClassifier

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Input
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import to_categorical

import joblib
print("Library Loaded")

# 2. LOAD DATASET

print("Loading dataset...")
df = pd.read_csv("dataset-gempa.csv")
print("Dataset Loaded:", df.shape)
df.head()

# 3. PREPROCESSING

print("Preprocessing...")

# Filter tahun
df = df[(df["year"] >= 2020) & (df["year"] <= 2024)].reset_index(drop=True)

# Drop kolom tidak diperlukan
drop_cols = [
    "id","net","updated","place","type",
    "status","locationSource","magSource",
    "magType","nst"   # tidak ada kolom time di dataset kamu
]

df = df.drop(columns=[c for c in drop_cols if c in df.columns], errors="ignore")

# Membuat label depth_class dari depth
def depth_to_class(d):
    if d < 70:
        return 0
    elif d < 300:
        return 1
    else:
        return 2

df["depth_class"] = df["depth"].apply(depth_to_class)
print("Distribusi Depth Class:")
df["depth_class"].value_counts()

# 4. FEATURE SELECTION

feature_cols = [
    "year", "latitude", "longitude",
    "mag", "gap", "dmin", "rms",
    "horizontalError", "magError"
]

df = df.dropna(subset=feature_cols)

X = df[feature_cols].values
y = df["depth_class"].values

print("X shape:", X.shape)
print("y shape:", y.shape)

# 5. SCALING + TRAIN-TEST SPLIT

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

# Compute class weights (IMBALANCE FIX)
classes = np.unique(y_train)
class_weights_arr = compute_class_weight(
    class_weight="balanced",
    classes=classes,
    y=y_train
)
class_weights = dict(zip(classes, class_weights_arr))

print("âš– Class Weights:", class_weights)

# 6. TRAIN XGBOOST

print("\n Training XGBoost...")

sample_weight = np.array([class_weights[i] for i in y_train])

xgb = XGBClassifier(
    n_estimators=300,
    max_depth=6,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    objective="multi:softprob",
    num_class=3,
    eval_metric="mlogloss",
    random_state=42
)

xgb.fit(X_train, y_train, sample_weight=sample_weight)
y_pred_xgb = xgb.predict(X_test)

print("\nXGBoost Accuracy:", accuracy_score(y_test, y_pred_xgb))
print("\nClassification Report:\n", classification_report(y_test, y_pred_xgb))

cm_xgb = confusion_matrix(y_test, y_pred_xgb)
sns.heatmap(cm_xgb, annot=True, fmt="d", cmap="Greens")
plt.title("Confusion Matrix - XGBoost")
plt.show()

# 7. TRAIN LSTM

print("\n Training LSTM...")

# reshape untuk LSTM
X_train_lstm = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))
X_test_lstm = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))

y_train_cat = to_categorical(y_train, num_classes=3)
y_test_cat = to_categorical(y_test, num_classes=3)

model_lstm = Sequential([
    Input(shape=(1, X_train.shape[1])),
    LSTM(64, return_sequences=True),
    Dropout(0.3),
    LSTM(32),
    Dropout(0.3),
    Dense(32, activation="relu"),
    Dense(3, activation="softmax")
])

model_lstm.compile(
    optimizer="adam",
    loss="categorical_crossentropy",
    metrics=["accuracy"]
)

early_stop = EarlyStopping(
    monitor="val_loss",
    patience=5,
    restore_best_weights=True
)

history = model_lstm.fit(
    X_train_lstm, y_train_cat,
    validation_split=0.2,
    epochs=50,
    batch_size=64,
    class_weight=class_weights,
    callbacks=[early_stop],
    verbose=1
)

y_pred_lstm = np.argmax(model_lstm.predict(X_test_lstm), axis=1)

print("\nLSTM Accuracy:", accuracy_score(y_test, y_pred_lstm))
print("\nClassification Report:\n", classification_report(y_test, y_pred_lstm))

cm_lstm = confusion_matrix(y_test, y_pred_lstm)
sns.heatmap(cm_lstm, annot=True, fmt="d", cmap="Purples")
plt.title("Confusion Matrix - LSTM")
plt.show()

# 8. SAVE MODELS

print("\nSaving models...")

os.makedirs("models", exist_ok=True)

joblib.dump(scaler, "models/scaler.pkl")
joblib.dump(xgb, "models/xgb_depth_class.pkl")
model_lstm.save("models/lstm_depth_class.keras")

print("Models saved successfully!")